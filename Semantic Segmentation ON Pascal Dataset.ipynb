{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of VOC.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcaoDKT0DTlw"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  \n",
        "import urllib.request\n",
        "import os\n",
        "import numpy as np\n",
        "import tarfile\n",
        "import shutil\n",
        "from PIL import Image\n",
        "import glob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljyRKUqiDiGN",
        "outputId": "7b3f8ea1-3cbb-489f-bdea-b0b773d71c82"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device=torch.device('cuda:0')\n",
        "  print('Cuda')\n",
        "else:\n",
        "  device=torch.device('cpu')\n",
        "  print('cpu')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwGwm2dI-dCV"
      },
      "source": [
        "\n",
        "url='http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar'\n",
        "path='VOC'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOF1iChGDrru"
      },
      "source": [
        "def get_archive(path,url):\n",
        "  try:\n",
        "    os.mkdir(path)\n",
        "  except:\n",
        "    path=path\n",
        "  \n",
        "  filename='devkit'\n",
        "  urllib.request.urlretrieve(url,f\"{path}/{filename}.tar\")\n",
        " \n",
        "get_archive(path,url)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oQUo6YBDsfX"
      },
      "source": [
        "def extract(path):\n",
        "  tar_file=tarfile.open(f\"{path}/devkit.tar\")\n",
        "  tar_file.extractall('./')\n",
        "  tar_file.close()\n",
        "  shutil.rmtree(path)\n",
        "\n",
        "extract(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnIZ5AqNH1nx"
      },
      "source": [
        "VOC_COLORMAP = [\n",
        "    [0, 0, 0],\n",
        "    [128, 0, 0],\n",
        "    [0, 128, 0],\n",
        "    [128, 128, 0],\n",
        "    [0, 0, 128],\n",
        "    [128, 0, 128],\n",
        "    [0, 128, 128],\n",
        "    [128, 128, 128],\n",
        "    [64, 0, 0],\n",
        "    [192, 0, 0],\n",
        "    [64, 128, 0],\n",
        "    [192, 128, 0],\n",
        "    [64, 0, 128],\n",
        "    [192, 0, 128],\n",
        "    [64, 128, 128],\n",
        "    [192, 128, 128],\n",
        "    [0, 64, 0],\n",
        "    [128, 64, 0],\n",
        "    [0, 192, 0],\n",
        "    [128, 192, 0],\n",
        "    [0, 64, 128],\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDnGmCOUDuG_"
      },
      "source": [
        "class VocDataset(Dataset):\n",
        "  def __init__(self,dir,color_map):\n",
        "    self.root=os.path.join(dir,'VOCdevkit/VOC2012')\n",
        "    self.target_dir=os.path.join(self.root,'SegmentationClass')\n",
        "    self.images_dir=os.path.join(self.root,'JPEGImages')\n",
        "    file_list=os.path.join(self.root,'ImageSets/Segmentation/trainval.txt')\n",
        "    self.files = [line.rstrip() for line in tuple(open(file_list, \"r\"))]\n",
        "    self.color_map=color_map\n",
        "    # self.pallete=self.get_collors()\n",
        "\n",
        "\n",
        "  def convert_to_segmentation_mask(self,mask):\n",
        "  # This function converts color channels of semgentation masks to number of classes (21 in this case)\n",
        "  # Semantic Segmentation requires a segmentation mask to be a NumPy array with the shape [height, width, num_classes].\n",
        "  # Each channel in this mask should encode values for a single class. Pixel in a mask channel should have\n",
        "  # a value of 1.0 if the pixel of the image belongs to this class and 0.0 otherwise.\n",
        "    height, width = mask.shape[:2]\n",
        "    segmentation_mask = np.zeros((height, width, len(self.color_map)), dtype=np.float32)\n",
        "    for label_index, label in enumerate(self.color_map):\n",
        "          segmentation_mask[:, :, label_index] = np.all(mask == label, axis=-1).astype(float)\n",
        "    return segmentation_mask\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    image_id=self.files[index]\n",
        "    image_path=os.path.join(self.images_dir,f\"{image_id}.jpg\")\n",
        "    label_path=os.path.join(self.target_dir,f\"{image_id}.png\")\n",
        "    image=cv.imread(image_path)\n",
        "    image=cv.cvtColor(image,cv.COLOR_BGR2RGB)\n",
        "    image=cv.resize(image,(256,256))\n",
        "    image=torch.tensor(image).float()\n",
        "    label=cv.imread(label_path)\n",
        "    label=cv.cvtColor(label,cv.COLOR_BGR2RGB)\n",
        "    label=cv.resize(label,(256,256))\n",
        "    label = self.convert_to_segmentation_mask(label)\n",
        "    label=torch.tensor(label).float()\n",
        "    \n",
        "    return image,label\n",
        "\n",
        "\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.files)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InyZWpWyESZ6"
      },
      "source": [
        "data=VocDataset('/content',VOC_COLORMAP)\n",
        "# plt.imshow(data.__getitem__(226)[0]/255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Luzbq5e6EUVE"
      },
      "source": [
        "train_set,val_set=torch.utils.data.random_split(data,[int(len(data)*0.8),round(len(data)*0.2)])\n",
        "train_loader=DataLoader(train_set,batch_size=10,shuffle=True)\n",
        "val_loader=DataLoader(val_set,batch_size=10,shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlY9w6DTYLOs",
        "outputId": "14f6f1ef-2808-4e2c-b786-34ba3e362284"
      },
      "source": [
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-p53yi4xr\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-p53yi4xr\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.0) (0.10.0+cu102)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (1.9.0+cu102)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.0) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.0) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.0) (1.15.0)\n",
            "Building wheels for collected packages: segmentation-models-pytorch, efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.2.0-py3-none-any.whl size=88681 sha256=8db381cb36a2f77edf859a85a46108584d18cfa5f3c7d9512bf096cb3cd2ac05\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6k7p5fyt/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12419 sha256=85abba273273760ec57d03fb4843b4b2b721299170a492fbe22bd57cfefa44d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=cb5a20a10205cbb4c60d2c638696e7c07fcb178aa86bbaad472aaa02c9b99414\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built segmentation-models-pytorch efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.0 timm-0.4.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bw5SVjDwDdhd"
      },
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "model = smp.Unet(encoder_name='resnet18',classes=21,activation='softmax')\n",
        "model=model.to(device)\n",
        "criterion = smp.utils.losses.DiceLoss(eps=1.)\n",
        "metrics = smp.utils.metrics.IoU(eps=1.)\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SA96L57TOlxa",
        "outputId": "e2418dd7-4ffe-47aa-8ac4-b4ed0af7e450"
      },
      "source": [
        "def train(model,optim,loss_f,epochs):\n",
        "  \n",
        "  for epoch in tqdm(range(epochs)):\n",
        "    for (X_train,y_train) in train_loader:\n",
        "      X_train,y_train=X_train.to(device),y_train.to(device,dtype=torch.int64)\n",
        "      X_train = X_train.permute(0, 3, 1, 2)\n",
        "      y_train = y_train.permute(0, 3, 1, 2)\n",
        "      y_pred=model(X_train)\n",
        "      loss=loss_f(y_pred,y_train)\n",
        "\n",
        "      optim.zero_grad()\n",
        "      loss.backward()\n",
        "      optim.step()\n",
        "    ious=[]\n",
        "    val_losses=[]\n",
        "    with torch.no_grad():\n",
        "      for b,(X_test,y_test) in enumerate(val_loader):\n",
        "        X_test,y_test=X_test.to(device),y_test.to(device)\n",
        "        X_test = X_test.permute(0, 3, 1, 2)\n",
        "        y_test = y_test.permute(0, 3, 1, 2)\n",
        "        y_val=model(X_test)\n",
        "        val_loss=loss_f(y_val,y_test)\n",
        "        val_losses.append(val_loss)\n",
        "        iou_=metrics(y_val,y_test)\n",
        "        ious.append(iou_)\n",
        "      ious=torch.tensor(ious)\n",
        "      val_losses=torch.tensor(val_losses)\n",
        "\n",
        "    print(f\"epoch : {epoch:2} train_loss: {loss:10.4} , val_loss : {val_losses.mean():10.4} val_iou: {ious.mean()}\")\n",
        "\n",
        "\n",
        "train(model,optimizer,criterion,5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A/usr/local/lib/python3.7/dist-packages/segmentation_models_pytorch/base/modules.py:104: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  return self.activation(x)\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [03:44<14:56, 224.15s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  0 train_loss:     0.6489 , val_loss :     0.5817 val_iou: 0.44954782724380493\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 40%|████      | 2/5 [07:28<11:12, 224.30s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  1 train_loss:      0.224 , val_loss :     0.3055 val_iou: 0.6289107203483582\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 60%|██████    | 3/5 [11:13<07:28, 224.41s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  2 train_loss:     0.2568 , val_loss :     0.2641 val_iou: 0.6320332288742065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 80%|████████  | 4/5 [14:56<03:43, 223.86s/it]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  3 train_loss:     0.1777 , val_loss :     0.2497 val_iou: 0.6409472227096558\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "100%|██████████| 5/5 [18:39<00:00, 223.94s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch :  4 train_loss:     0.1746 , val_loss :      0.247 val_iou: 0.6415237188339233\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fw0fIyPVQo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}